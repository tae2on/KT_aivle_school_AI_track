{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":["8y26X2L3EecB"],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# PLM NSMC Finetuning"],"metadata":{"id":"i0bA2c4UEbei"}},{"cell_type":"markdown","source":["## 0. 미션\n","참조\n","- model: https://huggingface.co/klue/roberta-base\n","- data\n","  - https://github.com/e9t/nsmc\n","  - https://huggingface.co/datasets/e9t/nsmc\n","\n","미션\n","- klue/roberta-base 모델을 활용하여 문장의 감정을 긍정/부정으로 예측하는 감정분류 데이터셋인 NSMC를 fine-tuning을 해 보면서 HuggingFace 사용법 및 PLM 학습 방법을 습득합니다.\n","- GPU는 6.fine-tuning 과정에서만 사용 하셔도 됩니다."],"metadata":{"id":"8y26X2L3EecB"}},{"cell_type":"markdown","source":["## 1. 구글 드라이브 연결 (최초 한번만 실행)\n","- 구글 드라이브는 데이터 저장 및 학습 결과를 저장하기 위해서 사용합니다.\n","- 구글 드라이브는 colab이 최초 실행 또는 종료 후 실행된 경우 한번 만 연결하면 됩니다."],"metadata":{"id":"uuNKlv3pFm9V"}},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"oxh3YfT1GLxh","colab":{"base_uri":"https://localhost:8080/"},"outputId":"239e0871-61b7-4f95-d815-1d9d56c43b96","executionInfo":{"status":"ok","timestamp":1731040974780,"user_tz":-540,"elapsed":2535,"user":{"displayName":"황태언","userId":"02206114473988724459"}}},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"markdown","source":["## *2. 환경 (매번 필수 실행)\n","- 환경은 colab 세션을 처음 시작하거나 다시 시작한 경우 실행되어야 합니다.\n","- 프로젝트 진행에 필요한 환경을 설정합니다."],"metadata":{"id":"botOy_KdFhfP"}},{"cell_type":"markdown","source":["### 2.1. 라이브러리 Import"],"metadata":{"id":"cUaVEjBUHTeB"}},{"cell_type":"code","source":["# install datasets 라이브러리\n","!pip install -qq datasets"],"metadata":{"id":"7JbJhkymLc5Y","executionInfo":{"status":"ok","timestamp":1731040984568,"user_tz":-540,"elapsed":8275,"user":{"displayName":"황태언","userId":"02206114473988724459"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["import os\n","import numpy as np\n","import pandas as pd\n","from tqdm.auto import tqdm\n","\n","import torch\n","import torch.nn.functional as F\n","from tqdm.auto import tqdm\n","from datasets import load_dataset\n","from transformers import (AutoTokenizer,\n","                          AutoModelForSequenceClassification,\n","                          TrainingArguments,\n","                          Trainer)"],"metadata":{"id":"2pfHWwihFkne","executionInfo":{"status":"ok","timestamp":1731041023746,"user_tz":-540,"elapsed":39187,"user":{"displayName":"황태언","userId":"02206114473988724459"}}},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":["### 2.2. 환경정보 설정\n","- WORKSPACE\n","  - 학습 데이터 및 학습결과를 저장하기 위한 경로입니다.\n","  - 필요할 경우 적당한 경로로 변경할 수 있습니다.\n","  - 경로를 변경 할 경우 전체 경로에 공백이 포함되지 않도록 주의해 주세요."],"metadata":{"id":"m0RiT4MhJc0W"}},{"cell_type":"code","source":["WORKSPACE = '/content/drive/MyDrive/kt aivle/언어지능 딥러닝/nlp-practice'"],"metadata":{"id":"_3oWzRGwJHBj","executionInfo":{"status":"ok","timestamp":1731041023746,"user_tz":-540,"elapsed":14,"user":{"displayName":"황태언","userId":"02206114473988724459"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["!mkdir {WORKSPACE}/data"],"metadata":{"id":"LFFjXP8fo6u7","colab":{"base_uri":"https://localhost:8080/"},"outputId":"41361620-4617-49e6-d998-19a4952058c5","executionInfo":{"status":"ok","timestamp":1731041023746,"user_tz":-540,"elapsed":13,"user":{"displayName":"황태언","userId":"02206114473988724459"}}},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["mkdir: cannot create directory ‘/content/drive/MyDrive/kt’: File exists\n","mkdir: cannot create directory ‘aivle/언어지능’: No such file or directory\n","mkdir: cannot create directory ‘딥러닝/nlp-practice/data’: No such file or directory\n"]}]},{"cell_type":"code","source":["!ls {WORKSPACE}"],"metadata":{"id":"MwBEAyDaMMMK","colab":{"base_uri":"https://localhost:8080/"},"outputId":"f64036aa-78c9-45b9-9e33-0c39779007e4","executionInfo":{"status":"ok","timestamp":1731041024070,"user_tz":-540,"elapsed":332,"user":{"displayName":"황태언","userId":"02206114473988724459"}}},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["ls: cannot access 'aivle/언어지능': No such file or directory\n","ls: cannot access '딥러닝/nlp-practice': No such file or directory\n","/content/drive/MyDrive/kt:\n"]}]},{"cell_type":"markdown","source":["## 3. Dataset understanding\n","- Dataset의 동작 및 사용 방법을 이해하기 위한 과정입니다.\n","- https://huggingface.co/docs/datasets/quickstart"],"metadata":{"id":"NU2ryVxQKB2G"}},{"cell_type":"code","source":["# Hugging Face Hub에서 dataset 다운로드\n","# dataset = load_dataset(\"e9t/nsmc\", trust_remote_code=True)\n","dataset = load_dataset(\"e9t/nsmc\")"],"metadata":{"id":"uQHnHxQLM738","colab":{"base_uri":"https://localhost:8080/"},"outputId":"3b6b1f98-4c7c-46e9-bf0b-fe37af974044","executionInfo":{"status":"ok","timestamp":1731041026187,"user_tz":-540,"elapsed":2122,"user":{"displayName":"황태언","userId":"02206114473988724459"}}},"execution_count":7,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n","The secret `HF_TOKEN` does not exist in your Colab secrets.\n","To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n","You will be able to reuse this secret in all of your notebooks.\n","Please note that authentication is recommended but still optional to access public models or datasets.\n","  warnings.warn(\n"]}]},{"cell_type":"code","source":["# dataset 구조 확인\n","dataset"],"metadata":{"id":"A4Z7kQ39NMVp","colab":{"base_uri":"https://localhost:8080/"},"outputId":"780ad398-5f41-469b-8b23-368246498a7b","executionInfo":{"status":"ok","timestamp":1731041026188,"user_tz":-540,"elapsed":11,"user":{"displayName":"황태언","userId":"02206114473988724459"}}},"execution_count":8,"outputs":[{"output_type":"execute_result","data":{"text/plain":["DatasetDict({\n","    train: Dataset({\n","        features: ['id', 'document', 'label'],\n","        num_rows: 150000\n","    })\n","    test: Dataset({\n","        features: ['id', 'document', 'label'],\n","        num_rows: 50000\n","    })\n","})"]},"metadata":{},"execution_count":8}]},{"cell_type":"code","source":["# train features 확인\n","dataset['train'].features"],"metadata":{"id":"0HpPrXrGNSE4","colab":{"base_uri":"https://localhost:8080/"},"outputId":"dd3d61be-cb91-4196-d94b-dec4d05eab83","executionInfo":{"status":"ok","timestamp":1731041026188,"user_tz":-540,"elapsed":9,"user":{"displayName":"황태언","userId":"02206114473988724459"}}},"execution_count":9,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'id': Value(dtype='string', id=None),\n"," 'document': Value(dtype='string', id=None),\n"," 'label': ClassLabel(names=['negative', 'positive'], id=None)}"]},"metadata":{},"execution_count":9}]},{"cell_type":"code","source":["# train num_rows 확인\n","dataset['train'].num_rows"],"metadata":{"id":"EyI6FWoSNznI","colab":{"base_uri":"https://localhost:8080/"},"outputId":"172a24a9-d297-4134-f0e8-17ccc9ee5813","executionInfo":{"status":"ok","timestamp":1731041026188,"user_tz":-540,"elapsed":7,"user":{"displayName":"황태언","userId":"02206114473988724459"}}},"execution_count":10,"outputs":[{"output_type":"execute_result","data":{"text/plain":["150000"]},"metadata":{},"execution_count":10}]},{"cell_type":"code","source":["# train 10개 데이터만 확인\n","for i  in range(10):\n","    print(dataset['train'][i])"],"metadata":{"id":"uBm8rYHuN7WN","colab":{"base_uri":"https://localhost:8080/"},"outputId":"4ea7350b-c49c-44bb-fb48-b080d5aece4b","executionInfo":{"status":"ok","timestamp":1731041026188,"user_tz":-540,"elapsed":6,"user":{"displayName":"황태언","userId":"02206114473988724459"}}},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["{'id': '9976970', 'document': '아 더빙.. 진짜 짜증나네요 목소리', 'label': 0}\n","{'id': '3819312', 'document': '흠...포스터보고 초딩영화줄....오버연기조차 가볍지 않구나', 'label': 1}\n","{'id': '10265843', 'document': '너무재밓었다그래서보는것을추천한다', 'label': 0}\n","{'id': '9045019', 'document': '교도소 이야기구먼 ..솔직히 재미는 없다..평점 조정', 'label': 0}\n","{'id': '6483659', 'document': '사이몬페그의 익살스런 연기가 돋보였던 영화!스파이더맨에서 늙어보이기만 했던 커스틴 던스트가 너무나도 이뻐보였다', 'label': 1}\n","{'id': '5403919', 'document': '막 걸음마 뗀 3세부터 초등학교 1학년생인 8살용영화.ㅋㅋㅋ...별반개도 아까움.', 'label': 0}\n","{'id': '7797314', 'document': '원작의 긴장감을 제대로 살려내지못했다.', 'label': 0}\n","{'id': '9443947', 'document': '별 반개도 아깝다 욕나온다 이응경 길용우 연기생활이몇년인지..정말 발로해도 그것보단 낫겟다 납치.감금만반복반복..이드라마는 가족도없다 연기못하는사람만모엿네', 'label': 0}\n","{'id': '7156791', 'document': '액션이 없는데도 재미 있는 몇안되는 영화', 'label': 1}\n","{'id': '5912145', 'document': '왜케 평점이 낮은건데? 꽤 볼만한데.. 헐리우드식 화려함에만 너무 길들여져 있나?', 'label': 1}\n"]}]},{"cell_type":"markdown","source":["### Mission\n","- Hugging Face Hub 'supark/ko-stsb' dataset을 다운로드 하세요.\n","- train 데이터의 features, num_rows를 확인하세요.\n","- train 데이터 값 10개만 출력하세요."],"metadata":{"id":"MaF7duT_OlvZ"}},{"cell_type":"code","source":["# Hugging Face Hub에서 dataset 다운로드\n","# dataset = load_dataset('supark/ko-stdb', trust_remote_code=True)\n","dataset = load_dataset(\"supark/ko-stsb\")"],"metadata":{"id":"vvCyNnZ0OgnH","executionInfo":{"status":"ok","timestamp":1731041027304,"user_tz":-540,"elapsed":1119,"user":{"displayName":"황태언","userId":"02206114473988724459"}}},"execution_count":12,"outputs":[]},{"cell_type":"code","source":["# dataset 구조 확인\n","dataset"],"metadata":{"id":"rodHG9qsO8V_","colab":{"base_uri":"https://localhost:8080/"},"outputId":"9c2a72bb-2c81-41cf-bc12-f1bd9b49f0c3","executionInfo":{"status":"ok","timestamp":1731041027304,"user_tz":-540,"elapsed":12,"user":{"displayName":"황태언","userId":"02206114473988724459"}}},"execution_count":13,"outputs":[{"output_type":"execute_result","data":{"text/plain":["DatasetDict({\n","    train: Dataset({\n","        features: ['sentence1', 'sentence2', 'score'],\n","        num_rows: 5709\n","    })\n","    dev: Dataset({\n","        features: ['sentence1', 'sentence2', 'score'],\n","        num_rows: 1498\n","    })\n","    test: Dataset({\n","        features: ['sentence1', 'sentence2', 'score'],\n","        num_rows: 1378\n","    })\n","})"]},"metadata":{},"execution_count":13}]},{"cell_type":"code","source":["# train features 확인\n","dataset['train'].features"],"metadata":{"id":"f5VjMwWEO_Jx","colab":{"base_uri":"https://localhost:8080/"},"outputId":"272d9c35-4b06-444d-fd6b-47066466fa63","executionInfo":{"status":"ok","timestamp":1731041027304,"user_tz":-540,"elapsed":10,"user":{"displayName":"황태언","userId":"02206114473988724459"}}},"execution_count":14,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'sentence1': Value(dtype='string', id=None),\n"," 'sentence2': Value(dtype='string', id=None),\n"," 'score': Value(dtype='float64', id=None)}"]},"metadata":{},"execution_count":14}]},{"cell_type":"code","source":["# train num_rows 확인\n","dataset['train'].num_rows"],"metadata":{"id":"OGjfQ4RtPBdA","colab":{"base_uri":"https://localhost:8080/"},"outputId":"167661d4-9f75-4c91-93ed-e26cf0515128","executionInfo":{"status":"ok","timestamp":1731041027304,"user_tz":-540,"elapsed":8,"user":{"displayName":"황태언","userId":"02206114473988724459"}}},"execution_count":15,"outputs":[{"output_type":"execute_result","data":{"text/plain":["5709"]},"metadata":{},"execution_count":15}]},{"cell_type":"code","source":["# train 10개 데이터만 확인\n","for i in range(10):\n","    print(dataset['train'][i])"],"metadata":{"id":"28uZ_sc6PEWb","colab":{"base_uri":"https://localhost:8080/"},"outputId":"8a3f6594-ecd0-424d-ba4a-e501279bd868","executionInfo":{"status":"ok","timestamp":1731041027304,"user_tz":-540,"elapsed":7,"user":{"displayName":"황태언","userId":"02206114473988724459"}}},"execution_count":16,"outputs":[{"output_type":"stream","name":"stdout","text":["{'sentence1': '비행기가 이륙하고 있다.', 'sentence2': '비행기가 이륙하고 있다.', 'score': 1.0}\n","{'sentence1': '한 남자가 큰 플루트를 연주하고 있다.', 'sentence2': '남자가 플루트를 연주하고 있다.', 'score': 0.76}\n","{'sentence1': '한 남자가 피자에 치즈를 뿌려놓고 있다.', 'sentence2': '한 남자가 구운 피자에 치즈 조각을 뿌려놓고 있다.', 'score': 0.76}\n","{'sentence1': '세 남자가 체스를 하고 있다.', 'sentence2': '두 남자가 체스를 하고 있다.', 'score': 0.52}\n","{'sentence1': '한 남자가 첼로를 연주하고 있다.', 'sentence2': '자리에 앉은 남자가 첼로를 연주하고 있다.', 'score': 0.85}\n","{'sentence1': '몇몇 남자들이 싸우고 있다.', 'sentence2': '두 남자가 싸우고 있다.', 'score': 0.85}\n","{'sentence1': '남자가 담배를 피우고 있다.', 'sentence2': '남자가 스케이트를 타고 있다.', 'score': 0.1}\n","{'sentence1': '남자가 피아노를 치고 있다.', 'sentence2': '남자가 기타를 연주하고 있다.', 'score': 0.32}\n","{'sentence1': '한 남자가 기타를 치고 노래를 부르고 있다.', 'sentence2': '한 여성이 어쿠스틱 기타를 연주하고 노래를 부르고 있다.', 'score': 0.44000000000000006}\n","{'sentence1': '사람이 고양이를 천장에 던지고 있다.', 'sentence2': '사람이 고양이를 천장에 던진다.', 'score': 1.0}\n"]}]},{"cell_type":"markdown","source":["## 4. Tokenizer understanding\n","- Tokenizer의 동작 및 사용 방법을 이해하기 위한 과정입니다.\n","- https://huggingface.co/docs/tokenizers/quicktour"],"metadata":{"id":"pNuNzRRuPzdR"}},{"cell_type":"code","source":["# Hugging Face Hub에서 tokenizer 다운로드\n","tokenizer = AutoTokenizer.from_pretrained(\"klue/roberta-base\")"],"metadata":{"id":"z1eqcNZQPzdS","colab":{"base_uri":"https://localhost:8080/"},"outputId":"6b915c93-c086-4fa5-b4c9-c58b391ab7e9","executionInfo":{"status":"ok","timestamp":1731041027304,"user_tz":-540,"elapsed":5,"user":{"displayName":"황태언","userId":"02206114473988724459"}}},"execution_count":17,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n","  warnings.warn(\n"]}]},{"cell_type":"code","source":["# tokenizer 확인\n","tokenizer"],"metadata":{"id":"U3o4-mNRPzdT","colab":{"base_uri":"https://localhost:8080/"},"outputId":"d566b340-8840-4a01-d313-213905228e2d","executionInfo":{"status":"ok","timestamp":1731041027808,"user_tz":-540,"elapsed":507,"user":{"displayName":"황태언","userId":"02206114473988724459"}}},"execution_count":18,"outputs":[{"output_type":"execute_result","data":{"text/plain":["BertTokenizerFast(name_or_path='klue/roberta-base', vocab_size=32000, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '[CLS]', 'eos_token': '[SEP]', 'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n","\t0: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t1: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t2: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t3: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t4: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","}"]},"metadata":{},"execution_count":18}]},{"cell_type":"code","source":["# 동작 확인을 위한 문장\n","sentence1 = \"지미 카터: 제임스 얼 지미 카터 주니어(1924년 10월 1일~)는 민주당 출신 미국의 제39대 대통령 (1977-81)이다.\"\n","sentence2 = \"수학: 수학(math)은 수, 양, 구조, 공간, 변화 등의 개념을 다루는 학문이다.\""],"metadata":{"id":"MybJFuZeQ-C3","executionInfo":{"status":"ok","timestamp":1731041027808,"user_tz":-540,"elapsed":16,"user":{"displayName":"황태언","userId":"02206114473988724459"}}},"execution_count":19,"outputs":[]},{"cell_type":"code","source":["# tokenizer tokens 확인\n","tokens = tokenizer.tokenize(sentence1)\n","print(tokens)"],"metadata":{"id":"mlHgn9nrPzdT","colab":{"base_uri":"https://localhost:8080/"},"outputId":"eea71dfb-0f04-4cd4-c5f2-b566e4a41dc7","executionInfo":{"status":"ok","timestamp":1731041027808,"user_tz":-540,"elapsed":16,"user":{"displayName":"황태언","userId":"02206114473988724459"}}},"execution_count":20,"outputs":[{"output_type":"stream","name":"stdout","text":["['지미', '카터', ':', '제임스', '얼', '지미', '카터', '주니어', '(', '1924', '##년', '10', '##월', '1', '##일', '~', ')', '는', '민주당', '출신', '미국', '##의', '제', '##39', '##대', '대통령', '(', '1977', '-', '81', ')', '이다', '.']\n"]}]},{"cell_type":"code","source":["# tokens to ids\n","token_ids = tokenizer.convert_tokens_to_ids(tokens)\n","print(token_ids)"],"metadata":{"id":"7h61bfI5PzdT","colab":{"base_uri":"https://localhost:8080/"},"outputId":"29fc8e13-f9da-418c-bea7-19081a607454","executionInfo":{"status":"ok","timestamp":1731041027808,"user_tz":-540,"elapsed":14,"user":{"displayName":"황태언","userId":"02206114473988724459"}}},"execution_count":21,"outputs":[{"output_type":"stream","name":"stdout","text":["[19537, 20604, 30, 10073, 1411, 19537, 20604, 14577, 12, 23489, 2440, 3633, 2429, 21, 2210, 97, 13, 793, 4186, 4327, 3666, 2079, 1545, 24834, 2104, 3698, 12, 13528, 17, 7561, 13, 30651, 18]\n"]}]},{"cell_type":"code","source":["# ids to tokens\n","tokens = tokenizer.convert_ids_to_tokens(token_ids)\n","print(tokens)"],"metadata":{"id":"YfY0t-0oPzdT","colab":{"base_uri":"https://localhost:8080/"},"outputId":"241e28c7-8c23-488c-d110-69ec05cb4bd6","executionInfo":{"status":"ok","timestamp":1731041027808,"user_tz":-540,"elapsed":13,"user":{"displayName":"황태언","userId":"02206114473988724459"}}},"execution_count":22,"outputs":[{"output_type":"stream","name":"stdout","text":["['지미', '카터', ':', '제임스', '얼', '지미', '카터', '주니어', '(', '1924', '##년', '10', '##월', '1', '##일', '~', ')', '는', '민주당', '출신', '미국', '##의', '제', '##39', '##대', '대통령', '(', '1977', '-', '81', ')', '이다', '.']\n"]}]},{"cell_type":"code","source":["# [CLS], tokens ids, [SEP] 형식으로 변환\n","token_ids = tokenizer.encode(sentence1)\n","print(token_ids)"],"metadata":{"id":"nrH8YN0ZR3iP","colab":{"base_uri":"https://localhost:8080/"},"outputId":"7269a784-5a16-475a-e581-0fc7ea2e95d8","executionInfo":{"status":"ok","timestamp":1731041027808,"user_tz":-540,"elapsed":11,"user":{"displayName":"황태언","userId":"02206114473988724459"}}},"execution_count":23,"outputs":[{"output_type":"stream","name":"stdout","text":["[0, 19537, 20604, 30, 10073, 1411, 19537, 20604, 14577, 12, 23489, 2440, 3633, 2429, 21, 2210, 97, 13, 793, 4186, 4327, 3666, 2079, 1545, 24834, 2104, 3698, 12, 13528, 17, 7561, 13, 30651, 18, 2]\n"]}]},{"cell_type":"code","source":["# 실제 모델에 입력형식으로 변환\n","# 배열에 미니배치 형식으로 여러 문장을 넣을 수 있음\n","inputs = tokenizer([sentence1, sentence2],\n","                   padding=True,         # 길이가 짧은 문장에 PAD를 붙여서 길이를 맞춤\n","                   truncation=True,      # 길이가 너무 긴 문장을 잘라서 버림\n","                   max_length=256,       # 최대 token 길이\n","                   return_tensors=\"pt\")  # pytorch 형식으로 값 리턴\n","inputs"],"metadata":{"id":"R62xYxGSSVpk","colab":{"base_uri":"https://localhost:8080/"},"outputId":"c485431c-663f-4399-cb76-3a55d21a4b6a","executionInfo":{"status":"ok","timestamp":1731041027808,"user_tz":-540,"elapsed":10,"user":{"displayName":"황태언","userId":"02206114473988724459"}}},"execution_count":24,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'input_ids': tensor([[    0, 19537, 20604,    30, 10073,  1411, 19537, 20604, 14577,    12,\n","         23489,  2440,  3633,  2429,    21,  2210,    97,    13,   793,  4186,\n","          4327,  3666,  2079,  1545, 24834,  2104,  3698,    12, 13528,    17,\n","          7561,    13, 30651,    18,     2],\n","        [    0,  5193,    30,  5193,    12,    80, 16012,    13,  1497,  1295,\n","            16,  1402,    16,  3962,    16,  4101,    16,  3908,   886,  2079,\n","          4453,  2069,  5778,  2259,  6204, 28674,    18,     2,     1,     1,\n","             1,     1,     1,     1,     1]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n","        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n","        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0]])}"]},"metadata":{},"execution_count":24}]},{"cell_type":"markdown","source":["### Mission\n","- Hugging Face Hub에서 'klue/bert-base' tokenizer를 다운로드 하세요.\n","- 위 문장을 tokeinze 해 보세요."],"metadata":{"id":"Hx4DpeOePzdT"}},{"cell_type":"code","source":["# Hugging Face Hub에서 tokenizer 다운로드\n","tokenizer = AutoTokenizer.from_pretrained(\"klue/bert-base\")"],"metadata":{"id":"Dmt3BtCnTzR6","executionInfo":{"status":"ok","timestamp":1731041027808,"user_tz":-540,"elapsed":8,"user":{"displayName":"황태언","userId":"02206114473988724459"}}},"execution_count":25,"outputs":[]},{"cell_type":"code","source":["# tokenizer 확인\n","tokenizer"],"metadata":{"id":"6kFb6lv6TzR7","colab":{"base_uri":"https://localhost:8080/"},"outputId":"a075947f-b8e8-4a99-8b62-5618685c63e0","executionInfo":{"status":"ok","timestamp":1731041027808,"user_tz":-540,"elapsed":8,"user":{"displayName":"황태언","userId":"02206114473988724459"}}},"execution_count":26,"outputs":[{"output_type":"execute_result","data":{"text/plain":["BertTokenizerFast(name_or_path='klue/bert-base', vocab_size=32000, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n","\t0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t1: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t2: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t3: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","\t4: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n","}"]},"metadata":{},"execution_count":26}]},{"cell_type":"code","source":["# tokenizer tokens 확인\n","tokens = tokenizer.tokenize(sentence1)\n","print(tokens)"],"metadata":{"id":"JzyqEzDVTzR8","colab":{"base_uri":"https://localhost:8080/"},"outputId":"8790824d-78d4-41d7-ca82-9a7a62390db6","executionInfo":{"status":"ok","timestamp":1731041027808,"user_tz":-540,"elapsed":6,"user":{"displayName":"황태언","userId":"02206114473988724459"}}},"execution_count":27,"outputs":[{"output_type":"stream","name":"stdout","text":["['지미', '카터', ':', '제임스', '얼', '지미', '카터', '주니어', '(', '1924', '##년', '10', '##월', '1', '##일', '~', ')', '는', '민주당', '출신', '미국', '##의', '제', '##39', '##대', '대통령', '(', '1977', '-', '81', ')', '이다', '.']\n"]}]},{"cell_type":"code","source":["# tokens to ids\n","tokens_ids = tokenizer.convert_tokens_to_ids(tokens)\n","print(tokens_ids)"],"metadata":{"id":"na3dsWVgTzR8","colab":{"base_uri":"https://localhost:8080/"},"outputId":"262cadf3-2488-4413-fdc3-ccf38b9dda6d","executionInfo":{"status":"ok","timestamp":1731041027808,"user_tz":-540,"elapsed":4,"user":{"displayName":"황태언","userId":"02206114473988724459"}}},"execution_count":28,"outputs":[{"output_type":"stream","name":"stdout","text":["[19537, 20604, 30, 10073, 1411, 19537, 20604, 14577, 12, 23489, 2440, 3633, 2429, 21, 2210, 97, 13, 793, 4186, 4327, 3666, 2079, 1545, 24834, 2104, 3698, 12, 13528, 17, 7561, 13, 30651, 18]\n"]}]},{"cell_type":"code","source":["# ids to tokens\n","tokens = tokenizer.convert_ids_to_tokens(tokens_ids)\n","print(tokens)"],"metadata":{"id":"Y69QQSq1TzR8","colab":{"base_uri":"https://localhost:8080/"},"outputId":"187ae363-47f2-4263-e0e3-57d5e8fcf470","executionInfo":{"status":"ok","timestamp":1731041028285,"user_tz":-540,"elapsed":479,"user":{"displayName":"황태언","userId":"02206114473988724459"}}},"execution_count":29,"outputs":[{"output_type":"stream","name":"stdout","text":["['지미', '카터', ':', '제임스', '얼', '지미', '카터', '주니어', '(', '1924', '##년', '10', '##월', '1', '##일', '~', ')', '는', '민주당', '출신', '미국', '##의', '제', '##39', '##대', '대통령', '(', '1977', '-', '81', ')', '이다', '.']\n"]}]},{"cell_type":"code","source":["# tokens ids 형식으로 변환\n","tokens_ids = tokenizer.encode(sentence1)\n","print(tokens_ids)"],"metadata":{"id":"p3Zg7TdnTzR8","colab":{"base_uri":"https://localhost:8080/"},"outputId":"e1005363-3f09-44b7-d8ed-8ad7e3d31003","executionInfo":{"status":"ok","timestamp":1731041028285,"user_tz":-540,"elapsed":7,"user":{"displayName":"황태언","userId":"02206114473988724459"}}},"execution_count":30,"outputs":[{"output_type":"stream","name":"stdout","text":["[2, 19537, 20604, 30, 10073, 1411, 19537, 20604, 14577, 12, 23489, 2440, 3633, 2429, 21, 2210, 97, 13, 793, 4186, 4327, 3666, 2079, 1545, 24834, 2104, 3698, 12, 13528, 17, 7561, 13, 30651, 18, 3]\n"]}]},{"cell_type":"code","source":["# 실제 모델에 입력형식으로 변환\n","# 배열에 미니배치 형식으로 여러 문장을 넣을 수 있음\n","inputs = tokenizer([sentence1, sentence2],\n","                   padding=True,         # 길이가 짧은 문장에 PAD를 붙여서 길이를 맞춤\n","                   truncation=True,      # 길이가 너무 긴 문장을 잘라서 버림\n","                   max_length=256,       # 최대 token 길이를 256으로 설정\n","                   return_tensors=\"pt\")  # pytorch 형식으로 값 리턴\n","inputs"],"metadata":{"id":"TfEiREKvTzR8","colab":{"base_uri":"https://localhost:8080/"},"outputId":"a0cfe4cf-db7f-4b6f-e2da-c8dce742ab51","executionInfo":{"status":"ok","timestamp":1731041028285,"user_tz":-540,"elapsed":4,"user":{"displayName":"황태언","userId":"02206114473988724459"}}},"execution_count":31,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'input_ids': tensor([[    2, 19537, 20604,    30, 10073,  1411, 19537, 20604, 14577,    12,\n","         23489,  2440,  3633,  2429,    21,  2210,    97,    13,   793,  4186,\n","          4327,  3666,  2079,  1545, 24834,  2104,  3698,    12, 13528,    17,\n","          7561,    13, 30651,    18,     3],\n","        [    2,  5193,    30,  5193,    12,    80, 16012,    13,  1497,  1295,\n","            16,  1402,    16,  3962,    16,  4101,    16,  3908,   886,  2079,\n","          4453,  2069,  5778,  2259,  6204, 28674,    18,     3,     0,     0,\n","             0,     0,     0,     0,     0]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n","        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n","        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0]])}"]},"metadata":{},"execution_count":31}]},{"cell_type":"markdown","source":["## 5. Model understanding\n","- Model의 동작 및 사용 방법을 이해하기 위한 과정입니다."],"metadata":{"id":"-qgMktW1Uxpb"}},{"cell_type":"code","source":["# Hugging Face Hub에서 tokenizer 다운로드\n","tokenizer = AutoTokenizer.from_pretrained(\"klue/roberta-base\")"],"metadata":{"id":"XWUl6Ynkk8OU","executionInfo":{"status":"ok","timestamp":1731041028286,"user_tz":-540,"elapsed":3,"user":{"displayName":"황태언","userId":"02206114473988724459"}}},"execution_count":32,"outputs":[]},{"cell_type":"code","source":["# Hugging Face Hub에서 model 다운로드\n","model = AutoModelForSequenceClassification.from_pretrained(\"klue/roberta-base\",\n","                                                           num_labels=2)  # 예측할 class 개수"],"metadata":{"id":"gKGu5S3-Uxpc","colab":{"base_uri":"https://localhost:8080/"},"outputId":"1e175fe9-58d4-478e-c799-5b7992354857","executionInfo":{"status":"ok","timestamp":1731041029268,"user_tz":-540,"elapsed":985,"user":{"displayName":"황태언","userId":"02206114473988724459"}}},"execution_count":33,"outputs":[{"output_type":"stream","name":"stderr","text":["Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at klue/roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]}]},{"cell_type":"code","source":["# model 확인\n","model"],"metadata":{"id":"ePE3KWb2Uxpc","colab":{"base_uri":"https://localhost:8080/"},"outputId":"3050b212-9629-4b85-a7b5-3f34cf1f4475","executionInfo":{"status":"ok","timestamp":1731041029269,"user_tz":-540,"elapsed":7,"user":{"displayName":"황태언","userId":"02206114473988724459"}}},"execution_count":34,"outputs":[{"output_type":"execute_result","data":{"text/plain":["RobertaForSequenceClassification(\n","  (roberta): RobertaModel(\n","    (embeddings): RobertaEmbeddings(\n","      (word_embeddings): Embedding(32000, 768, padding_idx=1)\n","      (position_embeddings): Embedding(514, 768, padding_idx=1)\n","      (token_type_embeddings): Embedding(1, 768)\n","      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","      (dropout): Dropout(p=0.1, inplace=False)\n","    )\n","    (encoder): RobertaEncoder(\n","      (layer): ModuleList(\n","        (0-11): 12 x RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","    )\n","  )\n","  (classifier): RobertaClassificationHead(\n","    (dense): Linear(in_features=768, out_features=768, bias=True)\n","    (dropout): Dropout(p=0.1, inplace=False)\n","    (out_proj): Linear(in_features=768, out_features=2, bias=True)\n","  )\n",")"]},"metadata":{},"execution_count":34}]},{"cell_type":"code","source":["sentence1 = '아 더빙.. 진짜 짜증나네요 목소리'\n","sentence2 = '흠...포스터보고 초딩영화줄....오버연기조차 가볍지 않구나'"],"metadata":{"id":"oqVNq9RGlpJF","executionInfo":{"status":"ok","timestamp":1731041029269,"user_tz":-540,"elapsed":6,"user":{"displayName":"황태언","userId":"02206114473988724459"}}},"execution_count":35,"outputs":[]},{"cell_type":"code","source":["# 입력 생성\n","inputs = tokenizer([sentence1,\n","                    sentence2],\n","                   padding=True,         # 길이가 짧은 문장에 PAD를 붙여서 길이를 맞춤/ 행렬로 만들어야 하기 때문에 길이를 맞춰줘야함\n","                   truncation=True,      # 길이가 너무 긴 문장을 잘라서 버림\n","                   max_length=256,       # 최대 token 길이\n","                   return_tensors=\"pt\")  # pytorch 형식으로 값 리턴\n","inputs"],"metadata":{"id":"sz4bRXvqUxpc","colab":{"base_uri":"https://localhost:8080/"},"outputId":"e64f4f51-cec6-4eff-cf41-89f9a159f165","executionInfo":{"status":"ok","timestamp":1731041029269,"user_tz":-540,"elapsed":6,"user":{"displayName":"황태언","userId":"02206114473988724459"}}},"execution_count":36,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'input_ids': tensor([[    0,  1376,   831,  2604,    18,    18,  4229,  9801,  2075,  2203,\n","          2182,  4243,     2,     1,     1,     1,     1,     1,     1,     1,\n","             1,     1,     1,     1,     1],\n","        [    0,  1963,    18,    18,    18, 11811,  2178,  2088, 28883, 16516,\n","          2776,    18,    18,    18,    18, 10737,  2156,  2015,  2446,  2232,\n","          6758,  2118,  1380,  6074,     2]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","         0],\n","        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","         0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","         0],\n","        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1]])}"]},"metadata":{},"execution_count":36}]},{"cell_type":"code","source":["# 추론\n","# 미세조정(finetuning)을 하지 않았기 때문에 현재는 답변을 신뢰 할 수 없음)\n","results = model(**inputs)\n","results"],"metadata":{"id":"88ZsvLtPUxpc","colab":{"base_uri":"https://localhost:8080/"},"outputId":"901125f5-0e38-4461-b1cd-f599d339d389","executionInfo":{"status":"ok","timestamp":1731041031980,"user_tz":-540,"elapsed":2714,"user":{"displayName":"황태언","userId":"02206114473988724459"}}},"execution_count":37,"outputs":[{"output_type":"execute_result","data":{"text/plain":["SequenceClassifierOutput(loss=None, logits=tensor([[0.0352, 0.2267],\n","        [0.0439, 0.2172]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)"]},"metadata":{},"execution_count":37}]},{"cell_type":"code","source":["# logits 확인\n","results.logits\n","# [부정일가능성, 긍정일가능성]"],"metadata":{"id":"p8L8i9pInq7u","colab":{"base_uri":"https://localhost:8080/"},"outputId":"2b62e6da-3a0a-4029-e96b-02920306a8a2","executionInfo":{"status":"ok","timestamp":1731041031980,"user_tz":-540,"elapsed":5,"user":{"displayName":"황태언","userId":"02206114473988724459"}}},"execution_count":38,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[0.0352, 0.2267],\n","        [0.0439, 0.2172]], grad_fn=<AddmmBackward0>)"]},"metadata":{},"execution_count":38}]},{"cell_type":"markdown","source":["### Mission\n","- Hugging Face Hub에서 'klue/bert-base' model 다운로드 하세요.\n","- 위 문장을 추론해 보세요."],"metadata":{"id":"jShAubtzUxpd"}},{"cell_type":"code","source":["# Hugging Face Hub에서 tokenizer 다운로드\n","tokenizer = AutoTokenizer.from_pretrained(\"klue/bert-base\")"],"metadata":{"id":"Pvq6DlV2mbv_","executionInfo":{"status":"ok","timestamp":1731041031980,"user_tz":-540,"elapsed":4,"user":{"displayName":"황태언","userId":"02206114473988724459"}}},"execution_count":39,"outputs":[]},{"cell_type":"code","source":["# Hugging Face Hub에서 model 다운로드\n","model = AutoModelForSequenceClassification.from_pretrained(\"klue/bert-base\",\n","                                                           num_labels=2)  # 예측할 class 개수"],"metadata":{"id":"Qwn_UgFvmbwA","colab":{"base_uri":"https://localhost:8080/"},"outputId":"714c00a7-2f9a-4725-cf62-d290f716dc13","executionInfo":{"status":"ok","timestamp":1731041032929,"user_tz":-540,"elapsed":952,"user":{"displayName":"황태언","userId":"02206114473988724459"}}},"execution_count":40,"outputs":[{"output_type":"stream","name":"stderr","text":["Some weights of BertForSequenceClassification were not initialized from the model checkpoint at klue/bert-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]}]},{"cell_type":"code","source":["# model 확인\n","model"],"metadata":{"id":"iK0-pjhvmbwA","colab":{"base_uri":"https://localhost:8080/"},"outputId":"7257021e-bcd7-4954-c520-c3c34f32b0fe","executionInfo":{"status":"ok","timestamp":1731041032930,"user_tz":-540,"elapsed":6,"user":{"displayName":"황태언","userId":"02206114473988724459"}}},"execution_count":41,"outputs":[{"output_type":"execute_result","data":{"text/plain":["BertForSequenceClassification(\n","  (bert): BertModel(\n","    (embeddings): BertEmbeddings(\n","      (word_embeddings): Embedding(32000, 768, padding_idx=0)\n","      (position_embeddings): Embedding(512, 768)\n","      (token_type_embeddings): Embedding(2, 768)\n","      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      (dropout): Dropout(p=0.1, inplace=False)\n","    )\n","    (encoder): BertEncoder(\n","      (layer): ModuleList(\n","        (0-11): 12 x BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSdpaSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","    )\n","    (pooler): BertPooler(\n","      (dense): Linear(in_features=768, out_features=768, bias=True)\n","      (activation): Tanh()\n","    )\n","  )\n","  (dropout): Dropout(p=0.1, inplace=False)\n","  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",")"]},"metadata":{},"execution_count":41}]},{"cell_type":"code","source":["sentence1 = '아 더빙.. 진짜 짜증나네요 목소리'\n","sentence2 = '흠...포스터보고 초딩영화줄....오버연기조차 가볍지 않구나'"],"metadata":{"id":"n-0vd-rgmbwA","executionInfo":{"status":"ok","timestamp":1731041032930,"user_tz":-540,"elapsed":6,"user":{"displayName":"황태언","userId":"02206114473988724459"}}},"execution_count":42,"outputs":[]},{"cell_type":"code","source":["# 입력 생성\n","inputs = tokenizer([sentence1,\n","                    sentence2],\n","                   padding=True,         # 길이가 짧은 문장에 PAD를 붙여서 길이를 맞춤\n","                   truncation=True,      # 길이가 너무 긴 문장을 잘라서 버림\n","                   max_length=256,       # 최대 token 길이\n","                   return_tensors=\"pt\")  # pytorch 형식으로 값 리턴\n","inputs"],"metadata":{"id":"GehuP8BXmbwA","colab":{"base_uri":"https://localhost:8080/"},"outputId":"adcb8c0f-7b98-4729-d03b-d0bf45dd4edc","executionInfo":{"status":"ok","timestamp":1731041032930,"user_tz":-540,"elapsed":5,"user":{"displayName":"황태언","userId":"02206114473988724459"}}},"execution_count":43,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'input_ids': tensor([[    2,  1376,   831,  2604,    18,    18,  4229,  9801,  2075,  2203,\n","          2182,  4243,     3,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0],\n","        [    2,  1963,    18,    18,    18, 11811,  2178,  2088, 28883, 16516,\n","          2776,    18,    18,    18,    18, 10737,  2156,  2015,  2446,  2232,\n","          6758,  2118,  1380,  6074,     3]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","         0],\n","        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","         0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","         0],\n","        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1]])}"]},"metadata":{},"execution_count":43}]},{"cell_type":"code","source":["# 추론\n","# 미세조정(finetuning)을 하지 않았기 때문에 현재는 답변을 신뢰 할 수 없음)\n","results = model(**inputs)\n","results"],"metadata":{"id":"O1_Mekv0mbwB","colab":{"base_uri":"https://localhost:8080/"},"outputId":"68b0a7a7-bf19-4ef6-a665-647170fc4a61","executionInfo":{"status":"ok","timestamp":1731041039416,"user_tz":-540,"elapsed":6489,"user":{"displayName":"황태언","userId":"02206114473988724459"}}},"execution_count":44,"outputs":[{"output_type":"execute_result","data":{"text/plain":["SequenceClassifierOutput(loss=None, logits=tensor([[ 0.5628,  0.1741],\n","        [ 0.3606, -0.3096]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)"]},"metadata":{},"execution_count":44}]},{"cell_type":"code","source":["# logits 확인\n","results.logits\n","# 큰값을 선택하는 것 // ex)아 ~ 긍정이네 or 부정이네"],"metadata":{"id":"fXaqDZOMmoCQ","colab":{"base_uri":"https://localhost:8080/"},"outputId":"02ea1e68-c241-4ff2-fe1e-0a27737bdc6d","executionInfo":{"status":"ok","timestamp":1731041039416,"user_tz":-540,"elapsed":5,"user":{"displayName":"황태언","userId":"02206114473988724459"}}},"execution_count":45,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[ 0.5628,  0.1741],\n","        [ 0.3606, -0.3096]], grad_fn=<AddmmBackward0>)"]},"metadata":{},"execution_count":45}]},{"cell_type":"markdown","source":["## 6. Finetuning\n","- PLM finetuning 방법을 이해하기 위한 과정입니다.\n","- TODO 코드를 완성하고 학습을 완료 하세요."],"metadata":{"id":"ea30tM5kn2FI"}},{"cell_type":"code","source":["# TODO: Hugging Face Hub에서 dataset 다운로드\n","dataset = load_dataset(\"e9t/nsmc\")"],"metadata":{"id":"hqgcAWEEoRi9","executionInfo":{"status":"ok","timestamp":1731041039898,"user_tz":-540,"elapsed":486,"user":{"displayName":"황태언","userId":"02206114473988724459"}}},"execution_count":46,"outputs":[]},{"cell_type":"code","source":["# TODO: Hugging Face Hub에서 tokenizer 다운로드\n","tokenizer = AutoTokenizer.from_pretrained(\"klue/roberta-base\")"],"metadata":{"id":"NaFCI-7Vn2FJ","executionInfo":{"status":"ok","timestamp":1731041039898,"user_tz":-540,"elapsed":5,"user":{"displayName":"황태언","userId":"02206114473988724459"}}},"execution_count":47,"outputs":[]},{"cell_type":"code","source":["# TODO: Hugging Face Hub에서 model 다운로드\n","model = AutoModelForSequenceClassification.from_pretrained(\"klue/roberta-base\",\n","                                                           num_labels=2)  # 예측할 class 개수"],"metadata":{"id":"aon84yWjn2FJ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1731041040206,"user_tz":-540,"elapsed":312,"user":{"displayName":"황태언","userId":"02206114473988724459"}},"outputId":"24567b80-0a54-4d78-f44d-5cc6328ad1de"},"execution_count":48,"outputs":[{"output_type":"stream","name":"stderr","text":["Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at klue/roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]}]},{"cell_type":"code","source":["# dataset 선언\n","class NSMCDataset(torch.utils.data.Dataset):\n","    def __init__(self, dataset):\n","        super().__init__()\n","        self.dataset = dataset\n","\n","    def __len__(self):\n","        return len(self.dataset)\n","\n","    def __getitem__(self, idx):\n","        return self.dataset[idx]['document'], self.dataset[idx]['label']\n","\n","train_dataset = NSMCDataset(dataset['train'])\n","test_dataset = NSMCDataset(dataset['test'])"],"metadata":{"id":"jV25f8Eava02","executionInfo":{"status":"ok","timestamp":1731041040206,"user_tz":-540,"elapsed":2,"user":{"displayName":"황태언","userId":"02206114473988724459"}}},"execution_count":49,"outputs":[]},{"cell_type":"code","source":["# collator 선언\n","class NSMCCollator:\n","    def __init__(self, tokenizer, max_length=256):\n","        self.tokenizer = tokenizer\n","        self.max_length = max_length\n","\n","    def __call__(self, batch):\n","        texts, labels = zip(*batch)\n","\n","        inputs = self.tokenizer(\n","            texts,\n","            padding=True,\n","            truncation=True,\n","            max_length=self.max_length,\n","            return_tensors=\"pt\",\n","        )\n","\n","        return_value = {\n","            \"input_ids\": inputs[\"input_ids\"],\n","            \"attention_mask\": inputs[\"attention_mask\"],\n","            \"labels\": torch.tensor(labels, dtype=torch.long),\n","        }\n","\n","        return return_value\n","\n","nsmc_collator = NSMCCollator(tokenizer)"],"metadata":{"id":"ii6_XSbcrdEt","executionInfo":{"status":"ok","timestamp":1731041040206,"user_tz":-540,"elapsed":2,"user":{"displayName":"황태언","userId":"02206114473988724459"}}},"execution_count":50,"outputs":[]},{"cell_type":"code","source":["# epoch 당 step 수 계산\n","steps_per_epoch = int(np.ceil(len(dataset['train']) / 128))\n","steps_per_epoch"],"metadata":{"id":"9sEnuIQpphec","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1731041040735,"user_tz":-540,"elapsed":530,"user":{"displayName":"황태언","userId":"02206114473988724459"}},"outputId":"872576f8-be70-4495-d3bc-13ae088da583"},"execution_count":51,"outputs":[{"output_type":"execute_result","data":{"text/plain":["1172"]},"metadata":{},"execution_count":51}]},{"cell_type":"code","source":["# Train arguments 선언\n","training_args = TrainingArguments(\n","    output_dir=f\"{WORKSPACE}/checkpoint/klue-roberta-base\",\n","    overwrite_output_dir=True,\n","    per_device_train_batch_size=128,\n","    per_device_eval_batch_size=128,\n","    gradient_accumulation_steps=1,\n","    eval_accumulation_steps=1,\n","    learning_rate=5e-5,\n","    weight_decay=0.01, # 오버피팅을 방지하기 위한 값\n","    lr_scheduler_type='linear', # 0부터 lr 만큼 상승했다가 감소함\n","    warmup_steps=1000, # lr정점을 찍는 구간이 1000(x축)\n","    num_train_epochs=3,\n","    logging_strategy=\"epoch\", #epoch 당 한번씩 수행한다는 의미\n","    evaluation_strategy=\"epoch\",\n","    save_strategy=\"epoch\",\n","    save_total_limit=5,\n","    # bf16=config.fp16,\n","    # bf16_full_eval=config.fp16,\n","    fp16=True, # 빠르게 연산하기위함\n","    fp16_full_eval=True, # 빠르게 연산하기 위함\n","    half_precision_backend=True,\n","    load_best_model_at_end=True, # valid가 가장 낮은 상태의 최적의 상태를 로딩해달라라는 의미 train과 겹치는 지점?\n","    report_to=\"none\"\n",")"],"metadata":{"id":"nbQVZ56on2FJ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1731041040735,"user_tz":-540,"elapsed":5,"user":{"displayName":"황태언","userId":"02206114473988724459"}},"outputId":"4b65c1b3-ef39-4b41-97b7-1b5c718cd683"},"execution_count":52,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n","  warnings.warn(\n"]}]},{"cell_type":"code","source":["# trainer 선언\n","trainer = Trainer(\n","        model=model,\n","        args=training_args,\n","        train_dataset=train_dataset,\n","        eval_dataset=test_dataset,\n","        tokenizer=tokenizer,\n","        data_collator=nsmc_collator,\n","    )"],"metadata":{"id":"fK49jOymn2FJ","executionInfo":{"status":"ok","timestamp":1731041040735,"user_tz":-540,"elapsed":3,"user":{"displayName":"황태언","userId":"02206114473988724459"}}},"execution_count":53,"outputs":[]},{"cell_type":"code","source":["# train\n","trainer.train()"],"metadata":{"id":"DyzXn_r6n2FK"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 7. Evaluation\n","- PLM finetuning 후 평가 방법을 이해하기 위한 과정입니다.\n","- TODO 코드를 완성하고 학습을 완료 하세요."],"metadata":{"id":"gttT8B_e3dpC"}},{"cell_type":"code","source":["# TODO: Hugging Face Hub에서 dataset 다운로드\n","dataset = load_dataset(\"e9t/nsmc\")"],"metadata":{"id":"mMppqm2o3dpD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 저장된 결과 확인\n","!ls {WORKSPACE}/checkpoint/klue-roberta-base"],"metadata":{"id":"8W5YQZaB3nzl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# TODO: 가장 성능이 좋은 경로를 입력하세요.\n","best_fn = f\"{WORKSPACE}/checkpoint/klue-roberta-base/checkpoint-2344\"\n","# 뒷숫자 나주엥 확인해보기"],"metadata":{"id":"-uTxoCwa68Ne"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# TODO: 저장된 tokenizer 로드\n","tokenizer = AutoTokenizer.from_pretrained(best_fn)"],"metadata":{"id":"kZ0gghdt3dpD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# TODO: 저장된 model 로드\n","model = AutoModelForSequenceClassification.from_pretrained(best_fn,\n","                                                          num_labels=2) # 예측할 class 개수"],"metadata":{"id":"Sb4tuaNO3dpD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# dataset 선언\n","class NSMCDataset(torch.utils.data.Dataset):\n","    def __init__(self, dataset):\n","        super().__init__()\n","\n","        self.dataset = dataset\n","\n","    def __len__(self):\n","        return len(self.dataset)\n","\n","    def __getitem__(self, idx):\n","        return self.dataset[idx]['document'], self.dataset[idx]['label']\n","\n","train_dataset = NSMCDataset(dataset['train'])\n","test_dataset = NSMCDataset(dataset['test'])"],"metadata":{"id":"q_Q98Iyx3dpD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# collator 선언\n","class NSMCCollator:\n","    def __init__(self, tokenizer, max_length=256):\n","        self.tokenizer = tokenizer\n","        self.max_length = max_length\n","\n","    def __call__(self, batch):\n","        texts, labels = zip(*batch)\n","\n","        inputs = self.tokenizer(\n","            texts,\n","            padding=True,\n","            truncation=True,\n","            max_length=self.max_length,\n","            return_tensors=\"pt\",\n","        )\n","\n","        return_value = {\n","            \"input_ids\": inputs[\"input_ids\"],\n","            \"attention_mask\": inputs[\"attention_mask\"],\n","            \"labels\": torch.tensor(labels, dtype=torch.long),\n","        }\n","\n","        return return_value\n","\n","nsmc_collator = NSMCCollator(tokenizer)"],"metadata":{"id":"DizgJPAI3dpD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# test loader 생성\n","test_loader = torch.utils.data.DataLoader(\n","    test_dataset,\n","    batch_size=128,\n","    shuffle=False,\n","    collate_fn=nsmc_collator\n",")"],"metadata":{"id":"RDEnab5I3dpD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# gpu 사용 가능 여부 확인\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"],"metadata":{"id":"e1ZyTR6t8iN_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["total_correct_cnt = 0\n","total_sample_cnt = 0\n","\n","model.to(device)\n","model.eval()\n","\n","with torch.no_grad():\n","    for batch in tqdm(test_loader):\n","        output = model(\n","            input_ids=batch[\"input_ids\"].to(device),\n","            attention_mask=batch[\"attention_mask\"].to(device),\n","        )\n","        probs = torch.functional.F.softmax(output.logits, dim=-1)\n","        prob, pred = torch.max(probs, dim=-1)\n","\n","        correct_cnt = (pred.cpu() == batch[\"labels\"]).sum().item()\n","        sample_cnt = len(batch[\"labels\"])\n","\n","        total_correct_cnt += correct_cnt\n","        total_sample_cnt += sample_cnt\n","\n","print(f\"Test Accuracy: {total_correct_cnt / total_sample_cnt * 100:.2f}%\")\n","print(f\"Correct / Total: {total_correct_cnt} / {total_sample_cnt}\")"],"metadata":{"id":"TXBMtFO-3dpD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# idx -> label\n","idx2label = {0: '부정', 1: '긍정'}"],"metadata":{"id":"Kf-hO-Sr_VzR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 랜덤하게 10개 셈플 결과 확인\n","idxs = np.random.randint(0, dataset['test'].num_rows, 10)\n","\n","with torch.no_grad():\n","    for idx in idxs:\n","        document = dataset['test'][int(idx)]['document']\n","        # inputs 생성\n","        inputs = tokenizer(\n","                    document,\n","                    truncation=True,\n","                    max_length=256,\n","                    return_tensors=\"pt\",\n","                ).to(device)\n","        # 추론\n","        logit = model(**inputs).logits[0]\n","        prob = F.softmax(logit, dim=-1)\n","        # 가장 높은 확률을 정답으로 예측\n","        y = prob.argmax(dim=-1)\n","        # 출력\n","        print(f\"{idx2label[y.item()]}\\t{prob[y].item():.4f}\\t{document}\")"],"metadata":{"id":"UB0gqGER3dpD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 직접 입력을 받아서 긍정/부정 예측\n","while True:\n","    print(\"input> \", end=\"\")\n","    document = str(input())\n","    if len(document) == 0:\n","        break\n","    # TODO: inputs 생성\n","    # TODO: 추론\n","    # TODO: 가장 높은 확률을 정답으로 예측\n","    # 출력\n","    print(f\"{idx2label[y.item()]}\\t{prob[y].item():.4f}\\t{document}\")"],"metadata":{"id":"WtpNA-GYuvof"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"Qpu_OePs-Ujg"},"execution_count":null,"outputs":[]}]}