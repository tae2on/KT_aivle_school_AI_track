{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","collapsed_sections":["i0bA2c4UEbei","8y26X2L3EecB","v0wmtPlzEs1Q","uuNKlv3pFm9V","botOy_KdFhfP","cUaVEjBUHTeB","m0RiT4MhJc0W","NU2ryVxQKB2G","eTtY55mHSuMw","vzrC3PRcV3zL","FOtCtWFbSsl6","iAgMMfZGVMJs","t6jHfNM5Vkbd","vI4m-RHcZIfd","vvYWXER9FqRF","YKy8t72YhTUi","TqAr-R9dFILl","6OgeDwFsFILm","Cr758JhiFILm","d8SzvXk3FILm","H_hpvBDpZh2-","fxUQO91_gmRA","jSGDiqpmrXqn","SHBM05RHLTc8","Xhs39LWSgveI","sytXS8tHirfO"]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# LLM NSMC Finetuning"],"metadata":{"id":"i0bA2c4UEbei"}},{"cell_type":"markdown","source":["## 0. 미션\n","참조\n","- 정보: https://cloud.google.com/vertex-ai/docs/generative-ai/open-models/use-gemma?hl=ko\n","- 2b instruction tuning: https://huggingface.co/google/gemma-1.1-2b-it\n","- data\n","  - https://github.com/e9t/nsmc\n","  - https://huggingface.co/datasets/e9t/nsmc\n","\n","미션\n","* 구글에서 공개한 gemma-1.1-2b-it를 활용하여 문장의 감정을 긍정/부정으로 예측하는 감정분류 데이터셋인 NSMC를 fine-tuning을 해 보면서 LLM 동작 방법과 fine-tuning 방법을 학습합니다."],"metadata":{"id":"8y26X2L3EecB"}},{"cell_type":"markdown","source":["## 1. 라이브러리 설치 (최초 한번만 실행)\n","- 라이브러리는 colab이 최초 실행 또는 종료 후 실행된 경우 한번만 실행하면 됩니다.\n","- GPU 메모리 부족등의 이유로 colab 세션을 다시 시작한 경우는 설치할 필요 없습니다.\n","- colab 세션을 다시 시작하려면 '런타임' >> '세션 다시 시작'을 선택하세요."],"metadata":{"id":"v0wmtPlzEs1Q"}},{"cell_type":"code","execution_count":1,"metadata":{"id":"4dT1hV8-ER8a","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1731044660018,"user_tz":-540,"elapsed":69547,"user":{"displayName":"황태언","userId":"02206114473988724459"}},"outputId":"c2680f4a-2897-4924-cd7a-3109175bc059"},"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.1/44.1 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.0/10.0 MB\u001b[0m \u001b[31m28.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m333.2/333.2 kB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m30.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m480.6/480.6 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m179.3/179.3 kB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","gcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2024.9.0 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m122.4/122.4 MB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m310.2/310.2 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h"]}],"source":["!pip install -qq -U transformers accelerate\n","!pip install -qq datasets\n","!pip install -qq peft\n","!pip install -qq bitsandbytes\n","!pip install -qq trl"]},{"cell_type":"markdown","source":["## 2. 구글 드라이브 연결 (최초 한번만 실행)\n","- 구글 드라이브는 데이터 저장 및 학습 결과를 저장하기 위해서 사용합니다.\n","- 구글 드라이브는 colab이 최초 실행 또는 종료 후 실행된 경우 한번 만 연결하면 됩니다."],"metadata":{"id":"uuNKlv3pFm9V"}},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"oxh3YfT1GLxh","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1731044790554,"user_tz":-540,"elapsed":42502,"user":{"displayName":"황태언","userId":"02206114473988724459"}},"outputId":"647bdad2-9f22-4ebb-c246-14035b592ee4"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"markdown","source":["## *3. 환경 (매번 필수 실행)\n","- 환경은 colab 세션을 처음 시작하거나 다시 시작한 경우 실행되어야 합니다.\n","- 프로젝트 진행에 필요한 환경을 설정합니다."],"metadata":{"id":"botOy_KdFhfP"}},{"cell_type":"markdown","source":["### 3.1. 라이브러리 Import"],"metadata":{"id":"cUaVEjBUHTeB"}},{"cell_type":"code","source":["import os\n","import pandas as pd\n","from tqdm.auto import tqdm\n","\n","import torch\n","from datasets import load_dataset\n","from transformers import (AutoTokenizer,\n","                          AutoModelForCausalLM,\n","                          BitsAndBytesConfig,\n","                          pipeline,\n","                          TrainingArguments)\n","from peft import (LoraConfig,\n","                  PeftModel)\n","from trl import SFTTrainer"],"metadata":{"id":"2pfHWwihFkne","executionInfo":{"status":"ok","timestamp":1731044819968,"user_tz":-540,"elapsed":29416,"user":{"displayName":"황태언","userId":"02206114473988724459"}}},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":["### 3.2. 환경정보 설정\n","- HF_TOKEN:\n","  - Hugging Face 인증을 위한 Token\n","  - 아래 URL에 접속해서 'User Access Token'을 생성하고 복사해서 Token에 입력하세요.\n","  - https://huggingface.co/settings/tokens\n","- WORKSPACE\n","  - 학습 데이터 및 학습결과를 저장하기 위한 경로입니다.\n","  - 필요할 경우 적당한 경로로 변경할 수 있습니다.\n","  - 경로를 변경 할 경우 전체 경로에 공백이 포함되지 않도록 주의해 주세요.\n","- MODEL_ID\n","  - 이번 프로젝트를 위한 LLM 입니다.\n","  - 구글에서 공개한 gemma-2b를 Instruction tunned한 버전입니다.\n","  - https://huggingface.co/google/gemma-2b-it"],"metadata":{"id":"m0RiT4MhJc0W"}},{"cell_type":"code","source":["# access token을 복사하세요.\n","HF_TOKEN = \"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\""],"metadata":{"id":"fY99u6ksi7Wj","executionInfo":{"status":"ok","timestamp":1731045509030,"user_tz":-540,"elapsed":442,"user":{"displayName":"황태언","userId":"02206114473988724459"}}},"execution_count":17,"outputs":[]},{"cell_type":"code","source":["WORKSPACE = '/content/drive/MyDrive/kt aivle/언어지능 딥러닝/nlp-practice'\n","MODEL_ID = 'google/gemma-1.1-2b-it'"],"metadata":{"id":"_3oWzRGwJHBj","executionInfo":{"status":"ok","timestamp":1731045509339,"user_tz":-540,"elapsed":2,"user":{"displayName":"황태언","userId":"02206114473988724459"}}},"execution_count":18,"outputs":[]},{"cell_type":"markdown","source":["## 4. Gemma understanding (재시작 필요)\n","- Gemma의 동작 및 사용 방법을 이해하기 위한 과정입니다.\n","- 이 과정을 시작하기 전 colab 세션을 다시 시작하세요.\n","- colab 세션을 다시 시작해야 하는 이유는 LLM의 model의 크기가 너무 크기 때문에 GPU의 메모리를 초기화 하기 위해서 입니다."],"metadata":{"id":"NU2ryVxQKB2G"}},{"cell_type":"markdown","source":["### 4.1. model load with 4 bits\n","- 2B token을 가진 gemma를 그냥 로딩할 경우는 약 9G의 GPU vRAM이 필요합니다.\n","- 4bit 양자화를 할 경우 2.2G의 GPU vRAM 필요."],"metadata":{"id":"eTtY55mHSuMw"}},{"cell_type":"code","source":["# declare 4 bits quantize\n","quantization_config = BitsAndBytesConfig(\n","    load_in_4bit=True,\n","    bnb_4bit_quant_type=\"nf4\",\n","    bnb_4bit_compute_dtype=torch.float16\n",")\n","# load 4 bits model\n","model = AutoModelForCausalLM.from_pretrained(MODEL_ID,\n","                                             device_map='auto',\n","                                             quantization_config=quantization_config,\n","                                             token=HF_TOKEN)\n","# load tokenizer\n","tokenizer = AutoTokenizer.from_pretrained(MODEL_ID,\n","                                          token=HF_TOKEN)\n","tokenizer.padding_side = 'right'"],"metadata":{"id":"-parNbnOJ8rM","colab":{"base_uri":"https://localhost:8080/","height":427},"executionInfo":{"status":"error","timestamp":1731045510360,"user_tz":-540,"elapsed":310,"user":{"displayName":"황태언","userId":"02206114473988724459"}},"outputId":"954603d7-eb17-4fee-dad3-f76fe1808c64"},"execution_count":19,"outputs":[{"output_type":"stream","name":"stderr","text":["CUDA is required but not available for bitsandbytes. Please consider installing the multi-platform enabled version of bitsandbytes, which is currently a work in progress. Please check currently supported platforms and installation instructions at https://huggingface.co/docs/bitsandbytes/main/en/installation#multi-backend\n"]},{"output_type":"error","ename":"RuntimeError","evalue":"CUDA is required but not available for bitsandbytes. Please consider installing the multi-platform enabled version of bitsandbytes, which is currently a work in progress. Please check currently supported platforms and installation instructions at https://huggingface.co/docs/bitsandbytes/main/en/installation#multi-backend","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-19-d93702dcc45d>\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m )\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# load 4 bits model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m model = AutoModelForCausalLM.from_pretrained(MODEL_ID,\n\u001b[0m\u001b[1;32m      9\u001b[0m                                              \u001b[0mdevice_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'auto'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m                                              \u001b[0mquantization_config\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mquantization_config\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/auto/auto_factory.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    562\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_model_mapping\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    563\u001b[0m             \u001b[0mmodel_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_model_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_model_mapping\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 564\u001b[0;31m             return model_class.from_pretrained(\n\u001b[0m\u001b[1;32m    565\u001b[0m                 \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mmodel_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mhub_kwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    566\u001b[0m             )\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   3655\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3656\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhf_quantizer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3657\u001b[0;31m             hf_quantizer.validate_environment(\n\u001b[0m\u001b[1;32m   3658\u001b[0m                 \u001b[0mtorch_dtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch_dtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfrom_tf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfrom_tf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfrom_flax\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfrom_flax\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice_map\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3659\u001b[0m             )\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/quantizers/quantizer_bnb_4bit.py\u001b[0m in \u001b[0;36mvalidate_environment\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m         \u001b[0mbnb_multibackend_is_enabled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mis_bitsandbytes_multi_backend_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m         \u001b[0mvalidate_bnb_backend_availability\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraise_exception\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"from_tf\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"from_flax\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/integrations/bitsandbytes.py\u001b[0m in \u001b[0;36mvalidate_bnb_backend_availability\u001b[0;34m(raise_exception)\u001b[0m\n\u001b[1;32m    556\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mis_bitsandbytes_multi_backend_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    557\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_validate_bnb_multi_backend_availability\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraise_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 558\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_validate_bnb_cuda_backend_availability\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraise_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/integrations/bitsandbytes.py\u001b[0m in \u001b[0;36m_validate_bnb_cuda_backend_availability\u001b[0;34m(raise_exception)\u001b[0m\n\u001b[1;32m    534\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mraise_exception\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    535\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_msg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 536\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_msg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    537\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    538\u001b[0m         \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_msg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: CUDA is required but not available for bitsandbytes. Please consider installing the multi-platform enabled version of bitsandbytes, which is currently a work in progress. Please check currently supported platforms and installation instructions at https://huggingface.co/docs/bitsandbytes/main/en/installation#multi-backend"]}]},{"cell_type":"markdown","source":["### 4.2. pipeline\n","- https://huggingface.co/docs/transformers/main_classes/pipelines\n","- huggingface에서 inference를 쉽게 하기 위해 정의한 라이브러리."],"metadata":{"id":"vzrC3PRcV3zL"}},{"cell_type":"code","source":["pipe = pipeline(\"text-generation\",\n","                model=model,\n","                tokenizer=tokenizer,\n","                max_new_tokens=512)\n","pipe"],"metadata":{"id":"6v0OZYLUMOvP"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 4.3. gemma prompt\n","- https://huggingface.co/google/gemma-1.1-2b-it\n","- 아래와 같은 형식이 gemma의 prompt 형식 입니다.\n","```\n","<bos><start_of_turn>user\n","{content}<end_of_turn>\n","<start_of_turn>model\n","```\n","- NSMC 추론을 위한 프롬프트를 생성하는 과정입니다."],"metadata":{"id":"FOtCtWFbSsl6"}},{"cell_type":"code","source":["doc = \"\"\"엄청나게 즐거운 시간이었습니다. 강추!!!\"\"\""],"metadata":{"id":"xeUApEqfT2pN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["messages = [\n","    {\n","        \"role\": \"user\",\n","        \"content\": \"다음 문장은 영화리뷰입니다. 긍정 또는 부정으로 분류해주세요:\\n\\n{}\".format(doc)\n","    }\n","]\n","prompt = pipe.tokenizer.apply_chat_template(messages,\n","                                            tokenize=False,\n","                                            add_generation_prompt=True)"],"metadata":{"id":"B8wHVUqYNKJn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(prompt)"],"metadata":{"id":"2c8yMyxdT7i7"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 4.4. gemma inference\n","- 이전 단계에서 생성한 prompt를 이용해 추론하고 결과를 확인하는 과정입니다."],"metadata":{"id":"iAgMMfZGVMJs"}},{"cell_type":"code","source":["outputs = pipe(\n","    prompt,\n","    do_sample=True,\n","    temperature=0.2,\n","    top_k=50,\n","    top_p=0.95,\n","    add_special_tokens=True\n",")\n","outputs"],"metadata":{"id":"8arCVMAdT8w6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(outputs[0][\"generated_text\"])"],"metadata":{"id":"EXIDO8Q6Uwyg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(outputs[0][\"generated_text\"][len(prompt):])"],"metadata":{"id":"muhxBoJzU9L-"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 4.5. gemma chat\n","- chatbot 형식의 감정분류 예 입니다."],"metadata":{"id":"t6jHfNM5Vkbd"}},{"cell_type":"code","source":["def gen_prompt(pipe, doc):\n","    messages = [\n","        {\n","            \"role\": \"user\",\n","            \"content\": \"다음 문장은 영화리뷰입니다. 긍정 또는 부정으로 분류해주세요:\\n\\n{}\".format(doc)\n","        }\n","    ]\n","    prompt = pipe.tokenizer.apply_chat_template(messages,\n","                                                tokenize=False,\n","                                                add_generation_prompt=True)\n","    return prompt"],"metadata":{"id":"Jbyo56u8U_oh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def gen_response(pipe, doc):\n","    prompt = gen_prompt(pipe, doc)\n","\n","    outputs = pipe(\n","        prompt,\n","        do_sample=True,\n","        temperature=0.2,\n","        top_k=50,\n","        top_p=0.95,\n","        add_special_tokens=True\n","    )\n","    return outputs[0][\"generated_text\"][len(prompt):]"],"metadata":{"id":"KgpDQ7OXWNuY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["while True:\n","    doc = input('문장 > ')\n","    doc = doc.strip()\n","    if len(doc) == 0:\n","        break\n","    result = gen_response(pipe, doc)\n","    print(f'감정 > {result}\\n\\n')"],"metadata":{"id":"Ey87FSqeWuur"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 5. Gemma-2b Finetuning (재시작 필요)\n","-  Gemma-2b를 Finetuning 보는 과정입니다.\n","- 이 과정을 시작하기 전 colab 세션을 다시 시작하세요.\n","- colab 세션을 다시 시작해야 하는 이유는 LLM의 model의 크기가 너무 크기 때문에 GPU의 메모리를 초기화 하기 위해서 입니다."],"metadata":{"id":"vI4m-RHcZIfd"}},{"cell_type":"markdown","source":["### 5.1. 학습 및 테스트 데이터를 다운로드"],"metadata":{"id":"vvYWXER9FqRF"}},{"cell_type":"code","source":["# Hugging Face Hub에서 dataset 다운로드\n","dataset = load_dataset(\"e9t/nsmc\")\n","dataset"],"metadata":{"id":"ORdpV5avFuZd"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 5.2. 학습을 위한 prompt 생성 함수 정의"],"metadata":{"id":"YKy8t72YhTUi"}},{"cell_type":"code","source":["# 학습을 위한 prompt를 생성합니다.\n","def gen_train_prompt(example):\n","    prompt_list = []\n","    for i in range(len(example['document'])):\n","        doc = example['document'][i]\n","        label = '긍정' if example['label'][i] == 1 else '부정'\n","        prompt_list.append(r\"\"\"<bos><start_of_turn>user\n","다음 문장은 영화리뷰입니다. 긍정 또는 부정으로 분류해주세요:\n","\n","{}<end_of_turn>\n","<start_of_turn>model\n","{}<end_of_turn><eos>\"\"\".format(doc, label))\n","    return prompt_list"],"metadata":{"id":"_ET3-W0kHLBw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# prompt 동작을 확인합니다.\n","prompts = gen_train_prompt(dataset['train'][:2])\n","for prompt in prompts:\n","    print(prompt)\n","    print(\"*\" * 50)"],"metadata":{"id":"X7UD3tHngv83"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 5.3. Gemma를 로딩 (4bit)"],"metadata":{"id":"TqAr-R9dFILl"}},{"cell_type":"code","source":["# declare 4 bits quantize\n","quantization_config = BitsAndBytesConfig(\n","    load_in_4bit=True,\n","    bnb_4bit_quant_type=\"nf4\",\n","    bnb_4bit_compute_dtype=torch.float16\n",")\n","# load 4 bits model\n","model = AutoModelForCausalLM.from_pretrained(MODEL_ID,\n","                                             device_map='auto',\n","                                             quantization_config=quantization_config,\n","                                             token=HF_TOKEN)\n","# load tokenizer\n","tokenizer = AutoTokenizer.from_pretrained(MODEL_ID,\n","                                          token=HF_TOKEN)\n","tokenizer.padding_side = 'right'"],"metadata":{"id":"HyO_QaEsFILm"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 5.4. LoRA를 적용하기 위한 설정 정의"],"metadata":{"id":"6OgeDwFsFILm"}},{"cell_type":"code","source":["# lora config\n","lora_config = LoraConfig(\n","    r=6,\n","    lora_alpha = 8,\n","    lora_dropout = 0.05,\n","    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n","    task_type=\"CAUSAL_LM\",\n",")"],"metadata":{"id":"7Up-NnBGFILm"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 5.5. Trainer를 이용한 학습\n","- https://huggingface.co/docs/trl/sft_trainer\n","- https://huggingface.co/docs/trl/v0.8.4/en/sft_trainer#trl.SFTTrainer\n","- https://huggingface.co/docs/transformers/v4.39.3/en/main_classes/trainer#transformers.TrainingArguments\n","- tutorial의 설정 중 다음 내용만 변경하셔도 됩니다.\n","  - output_dir: {WORKSPACE}/checkpoint/gemma-2b\n","  - max_steps: 1000 ~ 2000 사이\n","  - logging_steps: 100"],"metadata":{"id":"Cr758JhiFILm"}},{"cell_type":"code","source":["# trainer 정의\n","trainer = SFTTrainer(\n","    model=model, # 학습할 모델\n","    train_dataset=dataset['train'],  # 학습할 데이터 셋\n","    max_seq_length=256,  # 최대 토큰 갯수\n","    args=TrainingArguments(\n","        output_dir=f\"{WORKSPACE}/checkpoint/gemma-2b\",\n","        # num_train_epochs = 1,  # epoc으로 할 경우 너무 많이 걸리 수 있음\n","        max_steps=1000,  # 학습 step 수 (1000 ~ 2000 사이)\n","        per_device_train_batch_size=2,  # gpu당 입력 batch_size\n","        gradient_accumulation_steps=4,  # gradient 누적 후 학습\n","        optim=\"paged_adamw_8bit\",  # optimizer (QLoRA)\n","        warmup_steps=500,  # learning rate warmup step\n","        learning_rate=1e-4,  # learning rate\n","        # bf16=True,  # bf16 사용 여부 (3090 이상에서 가능)\n","        fp16=True,  # fp16 사용 여부 (예전 GPU에서 사용 가능, T4)\n","        logging_steps=100,  # 얼마만에 한번 씩 중간 결과를 확인할 것인가?\n","        report_to=\"none\",  # W&B에 학습결과 공유 가능\n","    ),\n","    peft_config=lora_config,  # QLoRA config\n","    formatting_func=gen_train_prompt,  # 프롬프트 생성 함수\n",")"],"metadata":{"id":"F0J_oLa3FILm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# train\n","trainer.train()"],"metadata":{"id":"L4FZg34sFILm"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 5.6. LoRA의 학습된 weight 저장\n","- 전체 weight가 아닌 변화량만 저장하므로 매우 작은 용량을 차지합니다."],"metadata":{"id":"d8SzvXk3FILm"}},{"cell_type":"code","source":["# save lora (delta weight)\n","trainer.model.save_pretrained(f\"{WORKSPACE}/checkpoint/gemma-2b/final\")"],"metadata":{"id":"_R3uZTZJFILm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# original model load (before finetuned)\n","model = AutoModelForCausalLM.from_pretrained(MODEL_ID,\n","                                             device_map='auto',\n","                                             torch_dtype=torch.float16,\n","                                             token=HF_TOKEN)"],"metadata":{"id":"fzxspoqMxcJo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# merge : original + delta wieght\n","model = PeftModel.from_pretrained(model,\n","                                  f\"{WORKSPACE}/checkpoint/gemma-2b/final\",\n","                                  device_map='auto',\n","                                  torch_dtype=torch.float16,\n","                                  token=HF_TOKEN)\n","model = model.merge_and_unload()"],"metadata":{"id":"pYq-YpoXxfVE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# save fine-tunned model\n","model.save_pretrained(f\"{WORKSPACE}/checkpoint/gemma-2b/merged\")"],"metadata":{"id":"2gpOt6_8xl4k"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 저장된 결과 확인\n","!ls -lh {WORKSPACE}/checkpoint/gemma-2b"],"metadata":{"id":"ivGbXnWvFILn"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 6. Gemma-2b 평가 및 추론 실습 (재시작 필요)\n","- '5. Gemma-2b 학습 실습'을 완료한 후 결과를 평가해 보는 과정입니다.\n","- 이 과정을 시작하기 전 colab 세션을 다시 시작하세요.\n","- colab 세션을 다시 시작해야 하는 이유는 LLM의 model의 크기가 너무 크기 때문에 GPU의 메모리를 초기화 하기 위해서 입니다."],"metadata":{"id":"H_hpvBDpZh2-"}},{"cell_type":"markdown","source":["### 6.1. Test Data loading\n","- 평가를 위한 테스트 데이터를 로딩합니다."],"metadata":{"id":"fxUQO91_gmRA"}},{"cell_type":"code","source":["# Hugging Face Hub에서 dataset 다운로드\n","dataset = load_dataset(\"e9t/nsmc\")\n","dataset"],"metadata":{"id":"leRLYA4HgeG1"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 6.2. 테스트를 위한 프롬프트 생성 및 확인"],"metadata":{"id":"jSGDiqpmrXqn"}},{"cell_type":"code","source":["# 평가를 위한 prompt\n","def gen_test_prompt(example):\n","    prompt_list = []\n","    for i in range(len(example['document'])):\n","        doc = example['document'][i]\n","        prompt_list.append(r\"\"\"<bos><start_of_turn>user\n","다음 문장은 영화리뷰입니다. 긍정 또는 부정으로 분류해주세요:\n","\n","{}<end_of_turn>\n","<start_of_turn>model\n","\"\"\".format(doc))\n","    return prompt_list"],"metadata":{"id":"oz0vZANHrXqo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# prompt 동작을 확인합니다.\n","prompts = gen_test_prompt(dataset['test'][:2])\n","for prompt in prompts:\n","    print(prompt)\n","    print(\"*\" * 50)"],"metadata":{"id":"L_Q-diuMrXqp"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 6.3. 학습된 Gemma를 로딩 (4bit)\n","- '{WORKSPACE}/checkpoint/gemma-2b/merged'에 저장된 모델을 로딩합니다."],"metadata":{"id":"SHBM05RHLTc8"}},{"cell_type":"code","source":["model_fn = f\"{WORKSPACE}/checkpoint/gemma-2b/merged\""],"metadata":{"id":"HZ06Aj6ErIBD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# declare 4 bits quantize\n","quantization_config = BitsAndBytesConfig(\n","    load_in_4bit=True,\n","    bnb_4bit_quant_type=\"nf4\",\n","    bnb_4bit_compute_dtype=torch.float16\n",")\n","# load 4 bits model\n","model = AutoModelForCausalLM.from_pretrained(model_fn,\n","                                             device_map='auto',\n","                                             quantization_config=quantization_config,\n","                                             token=HF_TOKEN)\n","# load tokenizer\n","tokenizer = AutoTokenizer.from_pretrained(MODEL_ID,\n","                                          token=HF_TOKEN)\n","tokenizer.padding_side = 'right'"],"metadata":{"id":"BSQ1EC5Xqt9p"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 6.4. 파이프라인 정의 및 평가\n","- 평가는 1000개만 실행하세요. (시간이 오래 걸립니다.)"],"metadata":{"id":"Xhs39LWSgveI"}},{"cell_type":"code","source":["# pipeline 정의\n","pipe = pipeline(\"text-generation\",\n","                model=model,\n","                tokenizer=tokenizer,\n","                max_new_tokens=10)"],"metadata":{"id":"Sy6h6fygFILn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# infer\n","total_sample_cnt, total_correct_cnt = 0, 0\n","for example in tqdm(dataset['test'].iter(1)):\n","    label = '긍정' if example['label'][0] == 1 else '부정'\n","\n","    prompt = gen_test_prompt(example)\n","    outputs = pipe(\n","        prompt,\n","        do_sample=True,\n","        temperature=0.2,\n","        top_k=50,\n","        top_p=0.95,\n","        add_special_tokens=True\n","    )\n","    pred = outputs[0][0]['generated_text'][len(prompt[0]):]\n","    total_sample_cnt += 1\n","    total_correct_cnt += 1 if label == pred else 0\n","\n","    # print(example['document'][0], \":\", pred)\n","    # print('-' * 20)\n","\n","    if total_sample_cnt >= 1000:\n","        break\n","print(f\"Test Accuracy: {total_correct_cnt} / {total_sample_cnt} = {total_correct_cnt/total_sample_cnt:.4f}\")"],"metadata":{"id":"vkaERK2SFILn"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 6.5. gemma chat\n","- chatbot 형식의 감정분류 입니다."],"metadata":{"id":"sytXS8tHirfO"}},{"cell_type":"code","source":["def gen_prompt(pipe, doc):\n","    messages = [\n","        {\n","            \"role\": \"user\",\n","            \"content\": \"다음 문장은 영화리뷰입니다. 긍정 또는 부정으로 분류해주세요:\\n\\n{}\".format(doc)\n","        }\n","    ]\n","    prompt = pipe.tokenizer.apply_chat_template(messages,\n","                                                tokenize=False,\n","                                                add_generation_prompt=True)\n","    return prompt"],"metadata":{"id":"zyA1Li8QZlCU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def gen_response(pipe, doc):\n","    prompt = gen_prompt(pipe, doc)\n","\n","    outputs = pipe(\n","        prompt,\n","        do_sample=True,\n","        temperature=0.2,\n","        top_k=50,\n","        top_p=0.95,\n","        add_special_tokens=True\n","    )\n","    return outputs[0][\"generated_text\"][len(prompt):]"],"metadata":{"id":"RAqKA2fHjGp-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["while True:\n","    doc = input('문장 > ')\n","    doc = doc.strip()\n","    if len(doc) == 0:\n","        break\n","    result = gen_response(pipe, doc)\n","    print(f'감정 > {result}\\n\\n')"],"metadata":{"id":"yUJJYA_njJY5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"xl6PLOhZjMUT"},"execution_count":null,"outputs":[]}]}